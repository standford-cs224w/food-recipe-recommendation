{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphSAGE-based Recommender Model (SageRecModel) for Food Recipe Recommendation (V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.loader import LinkNeighborLoader, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flush():\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "flush()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph dataset loading\n",
    "\n",
    "In this step, we load the graphs already generated in the graph dataset generation step.    \n",
    "Please read the README.md to find how to generate the graph datasets. Since generating graph datasets is time consiming and memory intensive, we didn't add them here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_459881/193544470.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  user={ num_nodes=226570 },\n",
       "  recipe={ x=[231637, 3081] },\n",
       "  (user, rates, recipe)={\n",
       "    edge_index=[2, 770011],\n",
       "    edge_label=[192502, 1],\n",
       "    edge_label_index=[2, 192502],\n",
       "  },\n",
       "  (recipe, rev_rates, user)={ edge_index=[2, 770011] }\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_graph(file_path):\n",
    "    return torch.load(file_path)\n",
    "\n",
    "dataset_version = 1\n",
    "base_data_path = f\"../data/graph/v{dataset_version}\"\n",
    "\n",
    "train_graph = load_graph(f\"{base_data_path}/train_graph.pt\")\n",
    "validation_graph = load_graph(f\"{base_data_path}/validation_graph.pt\")\n",
    "test_graph = load_graph(f\"{base_data_path}/test_graph.pt\")\n",
    "\n",
    "train_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  user={ num_nodes=226570 },\n",
       "  recipe={ x=[231637, 3081] },\n",
       "  (user, rates, recipe)={\n",
       "    edge_index=[2, 1019131],\n",
       "    edge_label=[113236, 1],\n",
       "    edge_label_index=[2, 113236],\n",
       "  },\n",
       "  (recipe, rev_rates, user)={ edge_index=[2, 1019131] }\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train graph information: \n",
      "Numbder of nodes: 458207\n",
      "Numbder of edges: 1540022\n",
      "Metadata: (['user', 'recipe'], [('user', 'rates', 'recipe'), ('recipe', 'rev_rates', 'user')])\n",
      "Edge index: tensor([[  3106,    317,  16543,  ...,    541, 208023,    489],\n",
      "        [211809,   6600, 109688,  ...,  62108,  96459, 200804]])\n",
      "Recipe embeddings dimension:  3081\n"
     ]
    }
   ],
   "source": [
    "print(\"Train graph information: \")\n",
    "print(\"Numbder of nodes:\", train_graph.num_nodes)\n",
    "print(\"Numbder of edges:\", train_graph.num_edges)\n",
    "print(\"Metadata:\", train_graph.metadata())\n",
    "print(\"Edge index:\", train_graph['user', 'rates', 'recipe'].edge_index)\n",
    "print(\"Recipe embeddings dimension: \", train_graph['recipe'].x.size(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "In this step, we implement a heterogeneous GNN model for an edge rating prediction task in a recipe recommendation system. This model leverages the GraphSAGE architecture to encode node features into embeddings and a custom edge decoder to predict ratings between user and recipe nodes. The primary components of the model include the encoder, the decoder, and an integrated model structure with enhanced regularization and embedding initialization.\n",
    "\n",
    "### GNNEncoder\n",
    "This module encodes node features into embeddings using the GraphSAGE architecture.\n",
    "\n",
    "**Structure**:\n",
    "- `conv1`: The first SAGE convolutional layer, performing initial feature transformation.\n",
    "- `conv2`: The second SAGE convolutional layer, producing the final node embeddings.\n",
    "\n",
    "**Forward Pass**:\n",
    "- Takes node features `x` and edge connections `edge_index` as input.\n",
    "- Applies `conv1` with a ReLU activation for non-linearity.\n",
    "- Applies `conv2` to generate the final node embeddings.\n",
    "\n",
    "### EdgeDecoder\n",
    "The EdgeDecoder serves as the prediction head, decoding the node embeddings to predict edge labels (ratings).\n",
    "\n",
    "**Structure**:\n",
    "- `lin1`: A fully connected layer that combines the embeddings from both nodes in an edge.\n",
    "- `lin2`: A linear layer that outputs a scalar representing the predicted edge label, such as a rating.\n",
    "\n",
    "**Forward Pass**:\n",
    "- Extracts embeddings for connected nodes (e.g., user and recipe).\n",
    "- Concatenates these embeddings and applies `lin1` with a ReLU activation.\n",
    "- Passes the result through `lin2`, which outputs the predicted edge rating as a scalar.\n",
    "\n",
    "### SageRecModel\n",
    "The SageRecModel class integrates the encoder and decoder into a complete model tailored for edge rating prediction in a heterogeneous graph. It includes mechanisms for handling missing features for user nodes and regularizing the model during training.\n",
    "\n",
    "**Structure**:\n",
    "- **User Embeddings**: Users are represented with an embedding layer (`user_embedding`) since they lack explicit features. This embedding layer is initialized with normalized values and is learned during training.\n",
    "- **Recipe Feature Transformation**: A linear transformation layer (`recipe_transform`) is applied to recipe features, preparing them for use in the encoder.\n",
    "- **Encoder**: Instantiates the `GNNEncoder` and adapts it to heterogeneous graphs using `to_hetero`, allowing the model to process multiple types of nodes and edges.\n",
    "- **Decoder**: A custom `EdgeDecoder` for predicting edge labels based on node embeddings.\n",
    "- **Dropout Layer**: A dropout layer is applied to the transformed recipe embeddings to prevent overfitting and enhance generalization.\n",
    "\n",
    "**Forward Pass**:\n",
    "- Takes a dictionary of node features `x_dict`, an edge index dictionary `edge_index_dict`, and the edge label index `edge_label_index`.\n",
    "- Normalizes the `user` embeddings for stability.\n",
    "- Normalizes and transforms the `recipe` features, applying ReLU and dropout to enhance robustness.\n",
    "- Passes `x_dict` and `edge_index_dict` to the encoder to produce node embeddings (`z_dict`).\n",
    "- Uses the `decoder` to predict edge labels based on these embeddings, generating edge rating predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SageRecModel(\n",
       "  (user_embedding): Embedding(226570, 128)\n",
       "  (recipe_transform): Linear(in_features=3081, out_features=128, bias=True)\n",
       "  (encoder): GraphModule(\n",
       "    (conv1): ModuleDict(\n",
       "      (user__rates__recipe): SAGEConv((-1, -1), 128, aggr=mean)\n",
       "      (recipe__rev_rates__user): SAGEConv((-1, -1), 128, aggr=mean)\n",
       "    )\n",
       "    (conv2): ModuleDict(\n",
       "      (user__rates__recipe): SAGEConv((-1, -1), 128, aggr=mean)\n",
       "      (recipe__rev_rates__user): SAGEConv((-1, -1), 128, aggr=mean)\n",
       "    )\n",
       "  )\n",
       "  (decoder): EdgeDecoder(\n",
       "    (lin1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (lin2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_dim)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.lin2 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z = torch.cat([z_dict['user'][row], z_dict['recipe'][col]], dim=-1)\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "\n",
    "        return z.view(-1)\n",
    "\n",
    "class SageRecModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_users, item_feature_dim, args):\n",
    "        super().__init__()\n",
    "        self.user_embedding = torch.nn.Embedding(num_users, hidden_dim)\n",
    "        self.recipe_transform = torch.nn.Linear(item_feature_dim, hidden_dim)\n",
    "        self.encoder = GNNEncoder(hidden_dim, hidden_dim)\n",
    "        self.encoder = to_hetero(self.encoder, train_graph.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=args['dropout_rate'])\n",
    "        self._initialize_embeddings()\n",
    "\n",
    "    def _initialize_embeddings(self):\n",
    "        torch.nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        self.user_embedding.weight.data = F.normalize(self.user_embedding.weight.data, p=2, dim=1)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        # Normalize user embeddings\n",
    "        x_dict['user'] = F.normalize(self.user_embedding.weight, p=2, dim=1)\n",
    "\n",
    "        # Normalize and transform recipe features and regularize.\n",
    "        x_dict['recipe'] = F.normalize(x_dict['recipe'], p=2, dim=1)\n",
    "        x_dict['recipe'] = self.recipe_transform(x_dict['recipe']).relu()\n",
    "        x_dict['recipe'] = self.dropout(x_dict['recipe'])  # Apply dropout\n",
    "\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "\n",
    "        return self.decoder(z_dict, edge_label_index)\n",
    "\n",
    "hidden_dim = 128 # Embedding dimension size.\n",
    "item_feature_dim = train_graph['recipe'].x.size(1)\n",
    "num_users = train_graph['user']['num_nodes']\n",
    "args = {\n",
    "    'dropout_rate': 0\n",
    "}\n",
    "model = SageRecModel(hidden_dim=hidden_dim, num_users=num_users, item_feature_dim=item_feature_dim, args=args).to(device)\n",
    "\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "In this step, we train our initialized \\texttt{SageRecModel} model to optimize for the edge rating prediction task, where we predict user ratings for recipes.\n",
    "\n",
    "### Mini-Batching\n",
    "To manage memory and computation on our large graph datasets, we use mini-batching, dividing the data into smaller, manageable subsets that the model processes sequentially. With PyG’s `LinkNeighborLoader`, we sample neighbors around each target edge, focusing on the local neighborhood of each user-recipe interaction. This approach enables the model to capture essential neighborhood context without loading the entire graph, making it highly efficient for large-scale training.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "- **Optimizer**: To optimize the model’s parameters, we use Adam optimizer that handles sparse gradients well, suitable for graph neural networks. Regularization is applied through weight decay, which discourages complex solutions and helps prevent overfitting, leading to better generalization.  \n",
    "To further stabilize training, we use a learning rate scheduler that dynamically adjusts the learning rate, reducing it gradually as the model approaches convergence. This prevents overshooting during optimization and enables fine-tuning for more accurate predictions.\n",
    "\n",
    "\n",
    "- **Loss Function**: Mean Squared Error (MSE) loss measures the difference between predicted and actual ratings. Given the continuous nature of ratings, MSE is a suitable choice for our link regression task and is calculated as:\n",
    "\n",
    "  $$\n",
    "  \\text{MSE Loss} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "\n",
    "  where $y_i$ is the true rating, $\\hat{y}_i$ is the predicted rating, and $N$ is the number of samples.\n",
    "\n",
    "\n",
    "### Training Process\n",
    "For each epoch, we perform the following steps:\n",
    "  1. Batch sampling: Sampling mini-batches of edges with the train data loader, allowing the model to process a portion of the data at each step.\n",
    "  2. Forward propagation: Generating predictions for each mini-batch and calculating the MSE loss based on the difference between predicted and actual ratings.\n",
    "  3. Backward Ppropagation: Updating the model’s parameters based on the computed gradients.\n",
    "  4. Parameter update: Updating the model’s parameters based on the computed gradients by optimizer.\n",
    "  5. Learning rate adjustment: Adjusting the learning rate periodically with the scheduler, stabilizing training as the model converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aref/ar_code/food-recipe-recommendation/venv/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    }
   ],
   "source": [
    "def create_link_neighbor_loader(data, edge_type, batch_size=1024, num_neighbors=[10, 10], shuffle=True, num_workers=4):\n",
    "    \"\"\"\n",
    "    Creates a LinkNeighborLoader for the specified edge type in a HeteroData object.\n",
    "\n",
    "    Parameters:\n",
    "    - data (HeteroData): The heterogeneous graph data.\n",
    "    - edge_type (tuple): The edge type for which to create the loader, e.g., ('user', 'rates', 'recipe').\n",
    "    - batch_size (int): Number of edges to include in each batch.\n",
    "    - num_neighbors (list): Number of neighbors to sample at each layer.\n",
    "    - shuffle (bool): Whether to shuffle the data.\n",
    "    - num_workers (int): Number of subprocesses to use for data loading.\n",
    "\n",
    "    Returns:\n",
    "    - loader (LinkNeighborLoader): The data loader for the specified edge type.\n",
    "    \"\"\"\n",
    "    # Ensure the edge_type exists in the data\n",
    "    if edge_type not in data.edge_types:\n",
    "        raise ValueError(f\"Edge type {edge_type} not found in the data.\")\n",
    "\n",
    "    # Access the edge_label_index and edge_label for the specified edge type\n",
    "    edge_label_index = data[edge_type].get('edge_label_index', data[edge_type].edge_index)\n",
    "    edge_label = data[edge_type].get('edge_label', None)\n",
    "\n",
    "    # Create the LinkNeighborLoader\n",
    "    loader = LinkNeighborLoader(\n",
    "        data=data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        edge_label_index=(edge_type, edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "edge_type = ('user', 'rates', 'recipe') # Define the edge type of interest\n",
    "batch_size = 512  # Adjust based on your GPU memory capacity\n",
    "num_neighbors = [10, 5] # Number of neighbors to sample at each layer\n",
    "num_workers = 4  # Adjust based on your system\n",
    "\n",
    "# Create the training data loader\n",
    "train_data_loader = create_link_neighbor_loader(\n",
    "    data=train_graph,\n",
    "    edge_type=edge_type,\n",
    "    batch_size=batch_size, \n",
    "    num_neighbors=num_neighbors, \n",
    "    shuffle=True,\n",
    "    num_workers=num_workers \n",
    ")\n",
    "\n",
    "weight_decay = 0.0001\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Implement a learning rate scheduler\n",
    "step_size = 10\n",
    "gamma = 0.1\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss (MSE): 2.1003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss (MSE): 1.5152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss (MSE): 1.5130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss (MSE): 1.5147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss (MSE): 1.5128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss (MSE): 1.5119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss (MSE): 1.5131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss (MSE): 1.5135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss (MSE): 1.5110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss (MSE): 1.5085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Loss (MSE): 1.5031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Loss (MSE): 1.5021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Loss (MSE): 1.5016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Loss (MSE): 1.5011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Loss (MSE): 1.5006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Loss (MSE): 1.5007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017, Loss (MSE): 1.5009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018, Loss (MSE): 1.5005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 019, Loss (MSE): 1.5002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss (MSE): 1.5009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def train(model, data_loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc='Training', unit='batch', leave=False):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch.x_dict, batch.edge_index_dict, batch['user', 'rates', 'recipe'].edge_label_index)\n",
    "        # Flatten target to match pred.\n",
    "        target = batch['user', 'rates', 'recipe'].edge_label.float().view(-1)\n",
    "        loss = F.mse_loss(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * target.size(0)\n",
    "    \n",
    "    # Compute average loss (MSE) per data point (edge). \n",
    "    mse = total_loss / len(data_loader.dataset)\n",
    "\n",
    "    # Step the scheduler to update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return mse\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train(model, train_data_loader, optimizer, scheduler)\n",
    "    print(f'Epoch: {epoch:03d}, Loss (MSE): {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Finally, we evaluate the performance of our model on the validation and test graphs using the **Root Mean Squared Error (RMSE)** and **@Recall@k** metrics. \n",
    "Although MSE is used as a loss function for training due to its efficient gradient properties, we use RMSE for evaluation because it provides error values in the same units as the target variable, making it more interpretable when assessing model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation': <torch.utils.data.dataloader.DataLoader at 0x7feb424b70d0>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x7feb424b7cd0>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class EdgeBatchDataset(Dataset):\n",
    "    def __init__(self, data, edge_type, batch_size):\n",
    "        \"\"\"\n",
    "        Dataset that batches only target edges and labels for evaluation.\n",
    "        The entire graph is shared across all batches.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.edge_label_index = data[edge_type].edge_label_index\n",
    "        self.edge_label = data[edge_type].edge_label\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.edge_label.size(0) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get batch range\n",
    "        start = idx * self.batch_size\n",
    "        end = min((idx + 1) * self.batch_size, self.edge_label.size(0))\n",
    "        \n",
    "        # Extract target edges and labels for this batch\n",
    "        edge_label_index = self.edge_label_index[:, start:end]\n",
    "        edge_label = self.edge_label[start:end]\n",
    "        \n",
    "        return edge_label_index, edge_label\n",
    "\n",
    "def create_evaluation_data_loader(data, edge_type, batch_size, num_workers):\n",
    "    \"\"\"\n",
    "    Creates a memory-efficient DataLoader for evaluation without cloning the graph.\n",
    "    \"\"\"\n",
    "    dataset = EdgeBatchDataset(data, edge_type, batch_size)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "    return loader\n",
    "\n",
    "\n",
    "\n",
    "# Create evaluation data loaders\n",
    "evaluation_data_loaders = {}\n",
    "evaluation_data_loaders[\"validation\"] = create_evaluation_data_loader(validation_graph, edge_type, batch_size, num_workers)\n",
    "evaluation_data_loaders[\"test\"] = create_evaluation_data_loader(test_graph, edge_type, batch_size, num_workers)\n",
    "\n",
    "evaluation_data_loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: RMSE = 1.3260, Recall@5 = 0.9645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: RMSE = 1.3074, Recall@5 = 0.9587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def encode_graph(model, graph, device):\n",
    "    \"\"\"\n",
    "    Precomputes node embeddings for the full graph.\n",
    "    \"\"\"\n",
    "    graph = graph.to(device)\n",
    "    x_dict = graph.x_dict\n",
    "    edge_index_dict = graph.edge_index_dict\n",
    "\n",
    "    # Normalize and precompute embeddings\n",
    "    x_dict['user'] = F.normalize(model.user_embedding.weight, p=2, dim=1)\n",
    "    x_dict['recipe'] = F.normalize(model.recipe_transform(x_dict['recipe']), p=2, dim=1)\n",
    "    return model.encoder(x_dict, edge_index_dict)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_rmse(decoder, z_dict, edge_label_index, edge_label, batch_size, device):\n",
    "    \"\"\"\n",
    "    Computes RMSE for edge predictions.\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    total_edges = 0\n",
    "\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    for i in tqdm(range(0, edge_label_index.size(1), batch_size), desc=\"RMSE\", leave=False):\n",
    "        batch_edge_index = edge_label_index[:, i:i+batch_size]\n",
    "        batch_edge_label = edge_label[i:i+batch_size].to(device)\n",
    "        \n",
    "        # Remove extra dimension to match predictions\n",
    "        batch_edge_label = batch_edge_label.view(-1)\n",
    "        \n",
    "        pred = decoder(z_dict, batch_edge_index).clamp(min=0, max=5)\n",
    "        total_loss += F.mse_loss(pred, batch_edge_label, reduction='sum').item()\n",
    "        total_edges += batch_edge_label.size(0)\n",
    "\n",
    "    return (total_loss / total_edges) ** 0.5\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_recall_at_k(decoder, z_dict, edge_label_index, edge_label, batch_size, k, relevance_threshold, device):\n",
    "    \"\"\"\n",
    "    Computes Recall@K for edge predictions.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    user_predictions = defaultdict(list)\n",
    "    user_true_items = defaultdict(set)\n",
    "\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    for i in tqdm(range(0, edge_label_index.size(1), batch_size), desc=\"Recall@K\", leave=False):\n",
    "        batch_edge_index = edge_label_index[:, i:i+batch_size]\n",
    "        batch_edge_label = edge_label[i:i+batch_size].to(device)\n",
    "        \n",
    "        # Remove extra dimension to match predictions\n",
    "        batch_edge_label = batch_edge_label.view(-1)\n",
    "        \n",
    "        pred = decoder(z_dict, batch_edge_index).clamp(min=0, max=5)\n",
    "\n",
    "        users, items = batch_edge_index\n",
    "        for j, user_id in enumerate(users.cpu().numpy()):\n",
    "            user_predictions[user_id].append((pred[j].item(), items[j].item()))\n",
    "            if batch_edge_label[j].item() >= relevance_threshold:\n",
    "                user_true_items[user_id].add(items[j].item())\n",
    "\n",
    "    recalls = []\n",
    "    for user_id in user_predictions:\n",
    "        top_k_pred_items = {item for _, item in sorted(user_predictions[user_id], key=lambda x: x[0], reverse=True)[:k]}\n",
    "        true_items = user_true_items[user_id]\n",
    "        if true_items:\n",
    "            recalls.append(len(top_k_pred_items & true_items) / len(true_items))\n",
    "    return sum(recalls) / len(recalls) if recalls else 0.0\n",
    "\n",
    "\n",
    "k = 5\n",
    "relevance_threshold = 4\n",
    "model.eval()\n",
    "\n",
    "for split_name, graph in zip([\"Validation\", \"Test\"], [validation_graph, test_graph]):\n",
    "    z_dict = encode_graph(model, graph, device)\n",
    "\n",
    "    edge_label_index = graph['user', 'rates', 'recipe'].edge_label_index\n",
    "    edge_label = graph['user', 'rates', 'recipe'].edge_label.float()\n",
    "\n",
    "    rmse = calculate_rmse(model.decoder, z_dict, edge_label_index, edge_label, batch_size, device)\n",
    "    recall_at_k = calculate_recall_at_k(model.decoder, z_dict, edge_label_index, edge_label, batch_size, k, relevance_threshold, device)\n",
    "\n",
    "    print(f\"{split_name}: RMSE = {rmse:.4f}, Recall@{k} = {recall_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

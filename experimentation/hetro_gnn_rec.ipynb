{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GraphSAGE-based GNN Model for Food Recipe Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import LGConv, SAGEConv, GATv2Conv, to_hetero\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import SAGEConv, GATv2Conv, LGConv\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import LayerNorm, BatchNorm\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush():\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "flush()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph dataset loading\n",
    "\n",
    "In this step, we load the graphs already generated in the graph dataset generation step.    \n",
    "Since generating graph datasets is time consiming, we won't add them to each notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(file_path):\n",
    "    return torch.load(file_path)\n",
    "\n",
    "dataset_version = 1\n",
    "base_data_path = f\"../data/graph/v{dataset_version}\"\n",
    "\n",
    "train_graph = load_graph(f\"{base_data_path}/train_graph.pt\")\n",
    "validation_graph = load_graph(f\"{base_data_path}/validation_graph.pt\")\n",
    "test_graph = load_graph(f\"{base_data_path}/test_graph.pt\")\n",
    "metadata = train_graph.metadata()\n",
    "\n",
    "train_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train graph information: \")\n",
    "print(\"Number of nodes:\", train_graph.num_nodes)\n",
    "print(\"Number of edges:\", train_graph.num_edges)\n",
    "print(\"Metadata:\", train_graph.metadata())\n",
    "print(\"Edge index:\", train_graph['user', 'rates', 'recipe'].edge_index)\n",
    "print(\"Recipe embeddings dimension: \", train_graph['recipe'].x.size(1))\n",
    "print(\"Type of ('user', 'rates', 'recipe') edge index\", train_graph[('user', 'rates', 'recipe')].edge_index.dtype)  \n",
    "print(\"Type of ('user', 'rates', 'recipe') edge index: \", train_graph[('user', 'rates', 'recipe')].edge_label_index.dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract edge_label for 'user', 'rates', 'recipe'\n",
    "ratings = train_graph[('user', 'rates', 'recipe')].edge_label\n",
    "\n",
    "# Verify statistics of ratings\n",
    "print(\"Statistics of edge_label (rates) in train_graph:\")\n",
    "print(f\"Min rating: {ratings.min().item()}\")\n",
    "print(f\"Max rating: {ratings.max().item()}\")\n",
    "print(f\"Unique ratings: {ratings.unique().tolist()}\")\n",
    "print(f\"Total number of ratings: {ratings.size(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################################################\n",
    "# # Model Definition\n",
    "# ############################################################\n",
    "# class HeteroGNN(nn.Module):\n",
    "#     def __init__(self, metadata, hidden_channels=64, out_channels=1, model_type='sage', \n",
    "#                  num_layers=2, dropout=0.5, l2_reg=1e-5, normalize=True):\n",
    "#         super().__init__()\n",
    "#         self.metadata = metadata\n",
    "#         self.model_type = model_type.lower()\n",
    "#         self.hidden_channels = hidden_channels\n",
    "#         self.out_channels = out_channels\n",
    "#         self.num_layers = num_layers\n",
    "#         self.dropout = dropout\n",
    "#         self.l2_reg = l2_reg\n",
    "#         self.normalize = normalize\n",
    "\n",
    "#         # User embedding\n",
    "#         user_node_count = train_graph['user'].num_nodes\n",
    "#         self.user_emb = nn.Embedding(user_node_count, self.hidden_channels)\n",
    "#         nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "\n",
    "#         # Recipe features\n",
    "#         recipe_x_dim = train_graph['recipe'].x.size(-1)\n",
    "#         self.recipe_norm = BatchNorm(recipe_x_dim, affine=True) if self.normalize else nn.Identity()\n",
    "#         self.recipe_lin = nn.Linear(recipe_x_dim, self.hidden_channels)\n",
    "#         nn.init.xavier_uniform_(self.recipe_lin.weight)\n",
    "\n",
    "#         # Select GNN Layer\n",
    "#         if self.model_type == 'sage':\n",
    "#             self.conv_class = SAGEConv\n",
    "#         elif self.model_type == 'gat':\n",
    "#             self.conv_class = GATv2Conv\n",
    "#         elif self.model_type == 'lightgcn':\n",
    "#             self.conv_class = LGConv\n",
    "#         else:\n",
    "#             raise ValueError(\"model_type should be one of ['sage', 'gat', 'lightgcn']\")\n",
    "\n",
    "#         # GNN Layers\n",
    "#         self.convs = nn.ModuleList()\n",
    "#         if self.model_type in ['sage', 'gat']:\n",
    "#             for _ in range(num_layers):\n",
    "#                 self.convs.append(self.conv_class(self.hidden_channels, self.hidden_channels))\n",
    "#         else:\n",
    "#             # LightGCN doesn't require input/output dimensions\n",
    "#             for _ in range(num_layers):\n",
    "#                 self.convs.append(LGConv())\n",
    "\n",
    "#         # Dropout layer (applies only if dropout > 0)\n",
    "#         self.dropout_layer = nn.Dropout(dropout) if dropout > 0 else None\n",
    "\n",
    "#         # Prediction Layer:\n",
    "#         # - For LightGCN: rating = dot product\n",
    "#         # - For SAGE/GAT: use MLP\n",
    "#         if self.model_type in ['sage', 'gat']:\n",
    "#             self.predict_mlp = nn.Sequential(\n",
    "#                 nn.Linear(self.hidden_channels * 2, self.hidden_channels),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(self.hidden_channels, out_channels)\n",
    "#             )\n",
    "#         else:\n",
    "#             self.predict_mlp = None\n",
    "\n",
    "#     def forward(self, x_dict, edge_index_dict):\n",
    "#         # Replace user node features with embeddings\n",
    "#         x_dict['user'] = self.user_emb.weight\n",
    "#         x_dict['recipe'] = self.recipe_norm(x_dict['recipe'])\n",
    "#         x_dict['recipe'] = self.recipe_lin(x_dict['recipe'])\n",
    "\n",
    "#         # Message passing\n",
    "#         user_recipe_edges = edge_index_dict[('user', 'rates', 'recipe')]\n",
    "#         recipe_user_edges = edge_index_dict[('recipe', 'rev_rates', 'user')]\n",
    "\n",
    "#         for conv in self.convs:\n",
    "#             x_dict['user'] = conv(x_dict['user'], user_recipe_edges)\n",
    "#             x_dict['recipe'] = conv(x_dict['recipe'], recipe_user_edges)\n",
    "\n",
    "#             # Apply dropout if defined\n",
    "#             if self.dropout_layer:\n",
    "#                 x_dict['user'] = self.dropout_layer(x_dict['user'])\n",
    "#                 x_dict['recipe'] = self.dropout_layer(x_dict['recipe'])\n",
    "\n",
    "#         return x_dict\n",
    "\n",
    "#     def predict(self, user_emb, recipe_emb):\n",
    "#         if self.model_type == 'lightgcn':\n",
    "#             # LightGCN rating = dot product\n",
    "#             return (user_emb * recipe_emb).sum(dim=-1, keepdim=True)\n",
    "#         else:\n",
    "#             # For SAGE/GAT: use MLP\n",
    "#             combined = torch.cat([user_emb, recipe_emb], dim=-1)\n",
    "#             return self.predict_mlp(combined)\n",
    "\n",
    "#     def loss_l2_regularization(self):\n",
    "#         l2_loss = torch.sum(self.user_emb.weight**2)\n",
    "#         for name, param in self.named_parameters():\n",
    "#             if 'weight' in name and param.requires_grad:\n",
    "#                 l2_loss += torch.sum(param**2)\n",
    "#         return self.l2_reg * l2_loss\n",
    "\n",
    "# ############################################################\n",
    "# # Data Loader for Mini-Batching\n",
    "# ############################################################\n",
    "# train_loader = LinkNeighborLoader(\n",
    "#     data=train_graph,\n",
    "#     num_neighbors=[10, 5],\n",
    "#     edge_label_index=(('user', 'rates', 'recipe'), train_graph['user', 'rates', 'recipe'].edge_label_index),\n",
    "#     edge_label=train_graph['user', 'rates', 'recipe'].edge_label,\n",
    "#     batch_size=1024,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# ############################################################\n",
    "# # Training Setup\n",
    "# ############################################################\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model_type = 'sage'  # Change to 'sage', 'gat', or 'lightgcn' as needed\n",
    "# dropout = 0.0 if model_type == 'lightgcn' else 0.5\n",
    "# hidden_channels = 128\n",
    "# model = HeteroGNN(metadata=metadata, hidden_channels=hidden_channels, num_layers=2, model_type=model_type, dropout=dropout).to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# ############################################################\n",
    "# # Training Function\n",
    "# ############################################################\n",
    "# def train(model, loader, optimizer):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch in loader:\n",
    "#         batch = batch.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         x_dict = {\n",
    "#             'user': torch.arange(batch['user'].num_nodes, device=device),\n",
    "#             'recipe': batch['recipe'].x\n",
    "#         }\n",
    "\n",
    "#         out_dict = model(x_dict, batch.edge_index_dict)\n",
    "\n",
    "#         # Extract edge indices for rating prediction\n",
    "#         user_nodes = batch['user', 'rates', 'recipe'].edge_label_index[0]\n",
    "#         recipe_nodes = batch['user', 'rates', 'recipe'].edge_label_index[1]\n",
    "\n",
    "#         user_emb = out_dict['user'][user_nodes]\n",
    "#         recipe_emb = out_dict['recipe'][recipe_nodes]\n",
    "\n",
    "#         pred = model.predict(user_emb, recipe_emb)\n",
    "#         target = batch['user', 'rates', 'recipe'].edge_label.float()\n",
    "\n",
    "#         if target.dim() == 1:\n",
    "#             target = target.unsqueeze(-1)\n",
    "\n",
    "#         loss = criterion(pred, target)\n",
    "#         loss += model.loss_l2_regularization()\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item() * target.size(0)\n",
    "\n",
    "#     return total_loss / len(loader.dataset)\n",
    "\n",
    "# ############################################################\n",
    "# # Training Loop\n",
    "# ############################################################\n",
    "# num_epochs = 5\n",
    "# for epoch in range(1, num_epochs + 1):\n",
    "#     train_mse = train(model, train_loader, optimizer)\n",
    "#     scheduler.step(train_mse)\n",
    "#     print(f\"Epoch {epoch:03d}, Training MSE: {train_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphSage\n",
    "# Epoch 001, Training MSE: 2.6171\n",
    "# Epoch 002, Training MSE: 1.7348\n",
    "# Epoch 003, Training MSE: 1.7048\n",
    "# Epoch 004, Training MSE: 1.6856\n",
    "# Epoch 005, Training MSE: 1.6774\n",
    "\n",
    "# LightGCN\n",
    "# Epoch 001, Training MSE: 15.5694\n",
    "# Epoch 002, Training MSE: 14.0834\n",
    "# Epoch 003, Training MSE: 14.0645\n",
    "# Epoch 004, Training MSE: 13.9592\n",
    "# Epoch 005, Training MSE: 13.9545\n",
    "\n",
    "# GAT\n",
    "# Epoch 001, Training MSE: 2.4236\n",
    "# Epoch 002, Training MSE: 1.7843\n",
    "# Epoch 003, Training MSE: 1.7651\n",
    "# Epoch 004, Training MSE: 1.7506\n",
    "# Epoch 005, Training MSE: 1.7458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import SAGEConv, GATv2Conv, LGConv\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import BatchNorm\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "############################################################\n",
    "# Model Definition\n",
    "############################################################\n",
    "class HeteroGNN(nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels=64, out_channels=1, model_type='sage', \n",
    "                 num_layers=2, dropout=0.5, l2_reg=1e-5, normalize=True):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        self.model_type = model_type.lower()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.l2_reg = l2_reg\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # User embeddings\n",
    "        user_node_count = metadata[0].index('user') != -1 and train_graph['user'].num_nodes\n",
    "        self.user_emb = nn.Embedding(user_node_count, self.hidden_channels)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "\n",
    "        # Recipe features\n",
    "        recipe_x_dim = train_graph['recipe'].x.size(-1)\n",
    "        self.recipe_norm = BatchNorm(recipe_x_dim, affine=True) if self.normalize else nn.Identity()\n",
    "        self.recipe_lin = nn.Linear(recipe_x_dim, self.hidden_channels)\n",
    "        nn.init.xavier_uniform_(self.recipe_lin.weight)\n",
    "\n",
    "        # Select GNN layer based on model_type\n",
    "        if self.model_type == 'sage':\n",
    "            # GraphSAGE: typically mean aggregator (default) is used.\n",
    "            ConvClass = SAGEConv\n",
    "            conv_args = (self.hidden_channels, self.hidden_channels)\n",
    "        elif self.model_type == 'gat':\n",
    "            # GATv2: use multiple heads, no static attention problem.\n",
    "            # The original GAT paper often used multiple heads (e.g., 8 heads).\n",
    "            # Here we choose heads=4 and concat=False to keep output dimension stable.\n",
    "            # This is a reasonable adaptation staying close to GAT-style architectures.\n",
    "            ConvClass = GATv2Conv\n",
    "            conv_args = (self.hidden_channels, self.hidden_channels)\n",
    "            self.gat_heads = 4\n",
    "        elif self.model_type == 'lightgcn':\n",
    "            # LightGCN: no input/output dimensions needed, no nonlinearities, no features transformed.\n",
    "            # Just LGConv layers.\n",
    "            ConvClass = LGConv\n",
    "            conv_args = ()\n",
    "        else:\n",
    "            raise ValueError(\"model_type should be one of ['sage', 'gat', 'lightgcn']\")\n",
    "\n",
    "        # GNN Layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        if self.model_type == 'sage':\n",
    "            for _ in range(num_layers):\n",
    "                self.convs.append(ConvClass(*conv_args))\n",
    "        elif self.model_type == 'gat':\n",
    "            # For GATv2Conv, specify heads and concat=False for simplicity\n",
    "            # Both layers have the same dimension since concat=False.\n",
    "            for _ in range(num_layers):\n",
    "                self.convs.append(ConvClass(self.hidden_channels, self.hidden_channels, heads=self.gat_heads, concat=False))\n",
    "        else:  # lightgcn\n",
    "            for _ in range(num_layers):\n",
    "                self.convs.append(ConvClass())\n",
    "\n",
    "        # Dropout layer (applies only if dropout > 0)\n",
    "        self.dropout_layer = nn.Dropout(dropout) if dropout > 0 else None\n",
    "\n",
    "        # Prediction Layer:\n",
    "        # - For LightGCN: rating = dot product of final user & item embeddings.\n",
    "        # - For SAGE/GAT: use an MLP for rating prediction.\n",
    "        if self.model_type in ['sage', 'gat']:\n",
    "            self.predict_mlp = nn.Sequential(\n",
    "                nn.Linear(self.hidden_channels * 2, self.hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_channels, out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.predict_mlp = None\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Replace user node features with embeddings\n",
    "        x_user = self.user_emb.weight\n",
    "        x_recipe = self.recipe_norm(x_dict['recipe'])\n",
    "        x_recipe = self.recipe_lin(x_recipe)\n",
    "\n",
    "        # For LightGCN, we need to sum embeddings from all layers, including the initial one.\n",
    "        if self.model_type == 'lightgcn':\n",
    "            # Initial embeddings\n",
    "            user_emb_layers = [x_user]\n",
    "            recipe_emb_layers = [x_recipe]\n",
    "        else:\n",
    "            user_emb_layers = []\n",
    "            recipe_emb_layers = []\n",
    "\n",
    "        # Message passing\n",
    "        user_recipe_edges = edge_index_dict[('user', 'rates', 'recipe')]\n",
    "        recipe_user_edges = edge_index_dict[('recipe', 'rev_rates', 'user')]\n",
    "\n",
    "        x_u = x_user\n",
    "        x_r = x_recipe\n",
    "        for conv in self.convs:\n",
    "            x_u = conv(x_u, user_recipe_edges)\n",
    "            x_r = conv(x_r, recipe_user_edges)\n",
    "\n",
    "            # Dropout if applicable\n",
    "            if self.dropout_layer and self.model_type in ['sage', 'gat']:\n",
    "                x_u = self.dropout_layer(x_u)\n",
    "                x_r = self.dropout_layer(x_r)\n",
    "\n",
    "            if self.model_type == 'lightgcn':\n",
    "                # Accumulate embeddings from each layer for LightGCN\n",
    "                user_emb_layers.append(x_u)\n",
    "                recipe_emb_layers.append(x_r)\n",
    "\n",
    "        if self.model_type == 'lightgcn':\n",
    "            # Final embedding is the sum of all layer embeddings (including initial)\n",
    "            x_user_final = torch.stack(user_emb_layers, dim=0).mean(dim=0)  # LightGCN uses averaged sum\n",
    "            x_recipe_final = torch.stack(recipe_emb_layers, dim=0).mean(dim=0)\n",
    "        else:\n",
    "            x_user_final = x_u\n",
    "            x_recipe_final = x_r\n",
    "\n",
    "        out_dict = {\n",
    "            'user': x_user_final,\n",
    "            'recipe': x_recipe_final\n",
    "        }\n",
    "        return out_dict\n",
    "\n",
    "    def predict(self, user_emb, recipe_emb):\n",
    "        if self.model_type == 'lightgcn':\n",
    "            # LightGCN rating = dot product\n",
    "            return (user_emb * recipe_emb).sum(dim=-1, keepdim=True)\n",
    "        else:\n",
    "            # For SAGE/GAT: use MLP\n",
    "            combined = torch.cat([user_emb, recipe_emb], dim=-1)\n",
    "            return self.predict_mlp(combined)\n",
    "\n",
    "    def loss_l2_regularization(self):\n",
    "        l2_loss = torch.sum(self.user_emb.weight**2)\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and param.requires_grad:\n",
    "                l2_loss += torch.sum(param**2)\n",
    "        return self.l2_reg * l2_loss\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Data Loader for Mini-Batching\n",
    "############################################################\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_graph,\n",
    "    num_neighbors=[10, 5],\n",
    "    edge_label_index=(('user', 'rates', 'recipe'), train_graph['user', 'rates', 'recipe'].edge_label_index),\n",
    "    edge_label=train_graph['user', 'rates', 'recipe'].edge_label,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "############################################################\n",
    "# Training Setup\n",
    "############################################################\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Example usage:\n",
    "# For LightGCN (close to original): no dropout, no MLP, dot product rating\n",
    "model_type = 'lightgcn'  # choose 'sage', 'gat', or 'lightgcn'\n",
    "dropout = 0.0 if model_type == 'lightgcn' else 0.5\n",
    "hidden_channels = 128\n",
    "\n",
    "model = HeteroGNN(metadata=metadata, hidden_channels=hidden_channels, num_layers=2, model_type=model_type, dropout=dropout).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "############################################################\n",
    "# Training Function\n",
    "############################################################\n",
    "def train(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_dict = {\n",
    "            'user': torch.arange(batch['user'].num_nodes, device=device),\n",
    "            'recipe': batch['recipe'].x\n",
    "        }\n",
    "\n",
    "        out_dict = model(x_dict, batch.edge_index_dict)\n",
    "\n",
    "        # Extract edge indices for rating prediction\n",
    "        user_nodes = batch['user', 'rates', 'recipe'].edge_label_index[0]\n",
    "        recipe_nodes = batch['user', 'rates', 'recipe'].edge_label_index[1]\n",
    "\n",
    "        user_emb = out_dict['user'][user_nodes]\n",
    "        recipe_emb = out_dict['recipe'][recipe_nodes]\n",
    "\n",
    "        pred = model.predict(user_emb, recipe_emb)\n",
    "        target = batch['user', 'rates', 'recipe'].edge_label.float()\n",
    "\n",
    "        if target.dim() == 1:\n",
    "            target = target.unsqueeze(-1)\n",
    "\n",
    "        loss = criterion(pred, target)\n",
    "        loss += model.loss_l2_regularization()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * target.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "############################################################\n",
    "# Training Loop (Example)\n",
    "############################################################\n",
    "num_epochs = 5\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_mse = train(model, train_loader, optimizer)\n",
    "    scheduler.step(train_mse)\n",
    "    print(f\"Epoch {epoch:03d}, Training MSE: {train_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGCN:\n",
    "# Epoch 001, Training MSE: 9.5261\n",
    "# Epoch 002, Training MSE: 6.4682\n",
    "# Epoch 003, Training MSE: 6.3267\n",
    "# Epoch 004, Training MSE: 6.2442\n",
    "# Epoch 005, Training MSE: 6.2378"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "In this step, we train our initialized \\texttt{SageRecModel} model to optimize for the edge rating prediction task, where we predict user ratings for recipes.\n",
    "\n",
    "### Mini-Batching\n",
    "To manage memory and computation on our large graph datasets, we use mini-batching, dividing the data into smaller, manageable subsets that the model processes sequentially. With PyG’s `LinkNeighborLoader`, we sample neighbors around each target edge, focusing on the local neighborhood of each user-recipe interaction. This approach enables the model to capture essential neighborhood context without loading the entire graph, making it highly efficient for large-scale training.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "- **Optimizer**: To optimize the model’s parameters, we use Adam optimizer that handles sparse gradients well, suitable for graph neural networks. Regularization is applied through weight decay, which discourages complex solutions and helps prevent overfitting, leading to better generalization.  \n",
    "To further stabilize training, we use a learning rate scheduler that dynamically adjusts the learning rate, reducing it gradually as the model approaches convergence. This prevents overshooting during optimization and enables fine-tuning for more accurate predictions.\n",
    "\n",
    "\n",
    "- **Loss Function**: Mean Squared Error (MSE) loss measures the difference between predicted and actual ratings. Given the continuous nature of ratings, MSE is a suitable choice for our link regression task and is calculated as:\n",
    "\n",
    "  $$\n",
    "  \\text{MSE Loss} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "\n",
    "  where $y_i$ is the true rating, $\\hat{y}_i$ is the predicted rating, and $N$ is the number of samples.\n",
    "\n",
    "\n",
    "### Training Process\n",
    "For each epoch, we perform the following steps:\n",
    "  1. Batch sampling: Sampling mini-batches of edges with the train data loader, allowing the model to process a portion of the data at each step.\n",
    "  2. Forward propagation: Generating predictions for each mini-batch and calculating the MSE loss based on the difference between predicted and actual ratings.\n",
    "  3. Backward Ppropagation: Updating the model’s parameters based on the computed gradients.\n",
    "  4. Parameter update: Updating the model’s parameters based on the computed gradients by optimizer.\n",
    "  5. Learning rate adjustment: Adjusting the learning rate periodically with the scheduler, stabilizing training as the model converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_link_neighbor_loader(data, edge_type, batch_size, num_neighbors, shuffle, num_workers):\n",
    "#     \"\"\"\n",
    "#     Creates a LinkNeighborLoader for the specified edge type in a HeteroData object.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data (HeteroData): The heterogeneous graph data.\n",
    "#     - edge_type (tuple): The edge type for which to create the loader, e.g., ('user', 'rates', 'recipe').\n",
    "#     - batch_size (int): Number of edges to include in each batch.\n",
    "#     - num_neighbors (list): Number of neighbors to sample at each layer.\n",
    "#     - shuffle (bool): Whether to shuffle the data.\n",
    "#     - num_workers (int): Number of subprocesses to use for data loading.\n",
    "\n",
    "#     Returns:\n",
    "#     - loader (LinkNeighborLoader): The data loader for the specified edge type.\n",
    "#     \"\"\"\n",
    "#     # Ensure the edge_type exists in the data\n",
    "#     if edge_type not in data.edge_types:\n",
    "#         raise ValueError(f\"Edge type {edge_type} not found in the data.\")\n",
    "\n",
    "#     # Access the edge_label_index and edge_label for the specified edge type\n",
    "#     edge_label_index = data[edge_type].get('edge_label_index', data[edge_type].edge_index)\n",
    "#     edge_label = data[edge_type].get('edge_label', None)\n",
    "\n",
    "#     # Create the LinkNeighborLoader\n",
    "#     loader = LinkNeighborLoader(\n",
    "#         data=data,\n",
    "#         num_neighbors=num_neighbors,\n",
    "#         edge_label_index=(edge_type, edge_label_index),\n",
    "#         edge_label=edge_label,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=shuffle,\n",
    "#         num_workers=num_workers,\n",
    "#     )\n",
    "\n",
    "#     return loader\n",
    "\n",
    "\n",
    "# edge_type = ('user', 'rates', 'recipe') # Define the edge type of interest\n",
    "# batch_size = 512  # Adjust based on your GPU memory capacity\n",
    "# num_neighbors = [10, 5, 5] # Number of neighbors to sample at each layer\n",
    "# num_workers = 4  # Adjust based on your system\n",
    "\n",
    "# # Create the training data loader\n",
    "# train_data_loader = create_link_neighbor_loader(\n",
    "#     data=train_graph,\n",
    "#     edge_type=edge_type,\n",
    "#     batch_size=batch_size, \n",
    "#     num_neighbors=num_neighbors, \n",
    "#     shuffle=True,\n",
    "#     num_workers=num_workers \n",
    "# )\n",
    "\n",
    "# weight_decay = 0.0001\n",
    "# learning_rate = 0.001\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# # Implement a learning rate scheduler\n",
    "# step_size = 10\n",
    "# gamma = 0.1\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, data_loader, optimizer, scheduler):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch in tqdm(data_loader, desc='Training', unit='batch', leave=False):\n",
    "#         batch = batch.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         pred = model(batch.x_dict, batch.edge_index_dict, batch['user', 'rates', 'recipe'].edge_label_index)\n",
    "#         # Flatten target to match pred.\n",
    "#         target = batch['user', 'rates', 'recipe'].edge_label.float().view(-1)\n",
    "#         loss = F.mse_loss(pred, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item() * target.size(0)\n",
    "    \n",
    "#     # Compute average loss (MSE) per data point (edge). \n",
    "#     mse = total_loss / len(data_loader.dataset)\n",
    "\n",
    "#     # Step the scheduler to update the learning rate\n",
    "#     scheduler.step()\n",
    "\n",
    "#     return mse\n",
    "\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 20\n",
    "# for epoch in range(1, num_epochs + 1):\n",
    "#     loss = train(model, train_data_loader, optimizer, scheduler)\n",
    "#     print(f'Epoch: {epoch:03d}, Loss (MSE): {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Finally, we evaluate the performance of our model on the validation and test graphs using the **Root Mean Squared Error (RMSE)** and **@Recall@k** metrics. \n",
    "Although MSE is used as a loss function for training due to its efficient gradient properties, we use RMSE for evaluation because it provides error values in the same units as the target variable, making it more interpretable when assessing model performance.\n",
    "\n",
    "We also report the evaluation results on the training graph to monitor the model error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class EdgeBatchDataset(Dataset):\n",
    "    def __init__(self, data, edge_type, batch_size):\n",
    "        \"\"\"\n",
    "        Dataset that batches only target edges and labels for evaluation.\n",
    "        The entire graph is shared across all batches.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.edge_label_index = data[edge_type].edge_label_index\n",
    "        self.edge_label = data[edge_type].edge_label\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.edge_label.size(0) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get batch range\n",
    "        start = idx * self.batch_size\n",
    "        end = min((idx + 1) * self.batch_size, self.edge_label.size(0))\n",
    "        \n",
    "        # Extract target edges and labels for this batch\n",
    "        edge_label_index = self.edge_label_index[:, start:end]\n",
    "        edge_label = self.edge_label[start:end]\n",
    "        \n",
    "        return edge_label_index, edge_label\n",
    "\n",
    "def create_evaluation_data_loader(data, edge_type, batch_size, num_workers):\n",
    "    \"\"\"\n",
    "    Creates a memory-efficient DataLoader for evaluation without cloning the graph.\n",
    "    \"\"\"\n",
    "    dataset = EdgeBatchDataset(data, edge_type, batch_size)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "    return loader\n",
    "\n",
    "\n",
    "edge_type = ('user', 'rates', 'recipe')\n",
    "batch_size = 1024\n",
    "num_neighbors = [10, 5] # Number of neighbors to sample at each layer\n",
    "num_workers = 4  # Adjust based on your system\n",
    "\n",
    "# Create evaluation data loaders\n",
    "evaluation_data_loaders = {}\n",
    "evaluation_data_loaders[\"validation\"] = create_evaluation_data_loader(validation_graph, edge_type, batch_size, num_workers)\n",
    "evaluation_data_loaders[\"test\"] = create_evaluation_data_loader(test_graph, edge_type, batch_size, num_workers)\n",
    "\n",
    "evaluation_data_loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_by_rmse(model, data_loader, full_data, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_edges = 0\n",
    "\n",
    "    for edge_label_index, edge_label in tqdm(data_loader, desc='Evaluating RMSE', leave=False):\n",
    "        edge_label_index = edge_label_index.squeeze(0).to(device)\n",
    "        edge_label = edge_label.squeeze(0).to(device).view(-1)  # Flatten target tensor\n",
    "\n",
    "        # Forward pass using the full graph\n",
    "        pred = model(full_data.x_dict, full_data.edge_index_dict, edge_label_index)\n",
    "        pred = pred.clamp(min=0, max=5)  # Clamp predictions\n",
    "\n",
    "        # Compute RMSE\n",
    "        total_loss += F.mse_loss(pred, edge_label, reduction='sum').item()\n",
    "        total_edges += edge_label.size(0)\n",
    "\n",
    "    return (total_loss / total_edges) ** 0.5\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_by_recall_at_k(model, data_loader, full_data, k, relevance_threshold):\n",
    "    model.eval()\n",
    "    user_predictions = defaultdict(list)\n",
    "    user_true_items = defaultdict(set)\n",
    "\n",
    "    for edge_label_index, edge_label in tqdm(data_loader, desc=f\"Evaluating Recall@{k}\", leave=False):\n",
    "        edge_label_index = edge_label_index.squeeze(0).to(device)\n",
    "        edge_label = edge_label.squeeze(0).to(device).view(-1)\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(full_data.x_dict, full_data.edge_index_dict, edge_label_index)\n",
    "        pred = pred.clamp(min=0, max=5)\n",
    "\n",
    "        # Extract user and item indices\n",
    "        users, items = edge_label_index\n",
    "\n",
    "        for i, user_id in enumerate(users.cpu().numpy()):\n",
    "            user_predictions[user_id].append((pred[i].item(), items[i].item()))\n",
    "            if edge_label[i].item() >= relevance_threshold:\n",
    "                user_true_items[user_id].add(items[i].item())\n",
    "\n",
    "    recalls = []\n",
    "    for user_id in user_predictions:\n",
    "        # Sort predictions by score in descending order and get top-k items\n",
    "        top_k_pred_items = {item for _, item in sorted(user_predictions[user_id], key=lambda x: x[0], reverse=True)[:k]}\n",
    "        true_items = user_true_items[user_id]\n",
    "\n",
    "        if true_items:\n",
    "            recall = len(top_k_pred_items & true_items) / len(true_items)\n",
    "            recalls.append(recall)\n",
    "\n",
    "    return sum(recalls) / len(recalls) if recalls else 0.0\n",
    "\n",
    "\n",
    "k = 5\n",
    "relevance_threshold = 4\n",
    "\n",
    "for data_split_name, data_loader in evaluation_data_loaders.items():\n",
    "    full_data = validation_graph if data_split_name == \"validation\" else test_graph\n",
    "    full_data = full_data.to(device)\n",
    "\n",
    "    rmse = evaluate_by_rmse(model, data_loader, full_data, device)\n",
    "    recall_at_k = evaluate_by_recall_at_k(model, data_loader, full_data, k, relevance_threshold)\n",
    "\n",
    "    print(f\"{data_split_name.capitalize()}: RMSE = {rmse:.4f}, Recall@{k} = {recall_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

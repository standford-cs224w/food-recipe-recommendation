{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Generic GNN Model Framework for Food Recipe Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import SAGEConv, GATv2Conv, LGConv, LayerNorm, BatchNorm, HeteroConv\n",
    "from torch_geometric.loader import LinkNeighborLoader, DataLoader, NodeLoader\n",
    "from torch_geometric.data import HeteroData, Dataset\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flush():\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "flush()\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.1\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric\n",
    "print(torch_geometric.__version__) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Dataset Retrieval\n",
    "\n",
    "In this step, we load the graphs already generated in the graph dataset generation step.  \n",
    "In order to generate the graph dataset files, please follow the instruction on README.md. Since generating graph datasets is time consiming, we won't add them to each notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_230364/2149098207.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  user={ num_nodes=226570 },\n",
       "  recipe={ x=[231637, 3081] },\n",
       "  (user, rates, recipe)={\n",
       "    edge_index=[2, 770011],\n",
       "    edge_label=[192502, 1],\n",
       "    edge_label_index=[2, 192502],\n",
       "  },\n",
       "  (recipe, rev_rates, user)={ edge_index=[2, 770011] }\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_graph(file_path):\n",
    "    return torch.load(file_path)\n",
    "\n",
    "dataset_version = 1\n",
    "base_data_path = f\"../data/graph/v{dataset_version}\"\n",
    "\n",
    "train_graph = load_graph(f\"{base_data_path}/train_graph.pt\")\n",
    "val_graph = load_graph(f\"{base_data_path}/validation_graph.pt\")\n",
    "test_graph = load_graph(f\"{base_data_path}/test_graph.pt\")\n",
    "\n",
    "train_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train graph information: \n",
      "Number of nodes: 458207\n",
      "Number of edges: 1540022\n",
      "Metadata: (['user', 'recipe'], [('user', 'rates', 'recipe'), ('recipe', 'rev_rates', 'user')])\n",
      "Edge index: tensor([[  3106,    317,  16543,  ...,    541, 208023,    489],\n",
      "        [211809,   6600, 109688,  ...,  62108,  96459, 200804]])\n",
      "Recipe node_embeddings dimension:  3081\n",
      "Type of ('user', 'rates', 'recipe') edge index torch.int64\n",
      "Type of ('user', 'rates', 'recipe') edge index:  torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train graph information: \")\n",
    "print(\"Number of nodes:\", train_graph.num_nodes)\n",
    "print(\"Number of edges:\", train_graph.num_edges)\n",
    "print(\"Metadata:\", train_graph.metadata())\n",
    "print(\"Edge index:\", train_graph['user', 'rates', 'recipe'].edge_index)\n",
    "print(\"Recipe node_embeddings dimension: \", train_graph['recipe'].x.size(1))\n",
    "print(\"Type of ('user', 'rates', 'recipe') edge index\", train_graph[('user', 'rates', 'recipe')].edge_index.dtype)  \n",
    "print(\"Type of ('user', 'rates', 'recipe') edge index: \", train_graph[('user', 'rates', 'recipe')].edge_label_index.dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of edge_label (rates) in train_graph:\n",
      "Min rating: 0\n",
      "Max rating: 5\n",
      "Unique ratings: [0, 1, 2, 3, 4, 5]\n",
      "Total number of ratings: 192502\n"
     ]
    }
   ],
   "source": [
    "# Extract edge_label for 'user', 'rates', 'recipe'\n",
    "ratings = train_graph[('user', 'rates', 'recipe')].edge_label\n",
    "\n",
    "# Verify statistics of ratings\n",
    "print(\"Statistics of edge_label (rates) in train_graph:\")\n",
    "print(f\"Min rating: {ratings.min().item()}\")\n",
    "print(f\"Max rating: {ratings.max().item()}\")\n",
    "print(f\"Unique ratings: {ratings.unique().tolist()}\")\n",
    "print(f\"Total number of ratings: {ratings.size(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node type 'user' is consistent across splits with 226570 nodes.\n",
      "Node type 'recipe' is consistent across splits with 231637 nodes.\n"
     ]
    }
   ],
   "source": [
    "def check_node_consistency(train_graph, val_graph, test_graph, node_types=['user', 'recipe']):\n",
    "    for node_type in node_types:\n",
    "        train_count = train_graph[node_type].num_nodes\n",
    "        val_count = val_graph[node_type].num_nodes\n",
    "        test_count = test_graph[node_type].num_nodes\n",
    "        if train_count != val_count or train_count != test_count:\n",
    "            print(f\"Node type '{node_type}' has inconsistent counts:\")\n",
    "            print(f\"  Train: {train_count}, Val: {val_count}, Test: {test_count}\")\n",
    "        else:\n",
    "            print(f\"Node type '{node_type}' is consistent across splits with {train_count} nodes.\")\n",
    "\n",
    "# Example usage:\n",
    "check_node_consistency(train_graph, val_graph, test_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating train_graph:\n",
      "Edge type ('user', 'rates', 'recipe') is valid.\n",
      "Edge type ('recipe', 'rev_rates', 'user') is valid.\n",
      "\n",
      "Validating val_graph:\n",
      "Edge type ('user', 'rates', 'recipe') is valid.\n",
      "Edge type ('recipe', 'rev_rates', 'user') is valid.\n",
      "\n",
      "Validating test_graph:\n",
      "Edge type ('user', 'rates', 'recipe') is valid.\n",
      "Edge type ('recipe', 'rev_rates', 'user') is valid.\n"
     ]
    }
   ],
   "source": [
    "def validate_edge_indices(graph, node_types=['user', 'recipe']):\n",
    "    for edge_type in graph.edge_types:\n",
    "        src_type, rel_type, dst_type = edge_type\n",
    "        src_max = graph[edge_type].edge_index[0].max().item()\n",
    "        dst_max = graph[edge_type].edge_index[1].max().item()\n",
    "        src_count = graph[src_type].num_nodes\n",
    "        dst_count = graph[dst_type].num_nodes\n",
    "        if src_max >= src_count:\n",
    "            print(f\"Edge type {edge_type}: Source index {src_max} out of bounds for node type '{src_type}' with {src_count} nodes.\")\n",
    "        if dst_max >= dst_count:\n",
    "            print(f\"Edge type {edge_type}: Destination index {dst_max} out of bounds for node type '{dst_type}' with {dst_count} nodes.\")\n",
    "        if src_max < src_count and dst_max < dst_count:\n",
    "            print(f\"Edge type {edge_type} is valid.\")\n",
    "\n",
    "# Example usage:\n",
    "print(\"Validating train_graph:\")\n",
    "validate_edge_indices(train_graph)\n",
    "print(\"\\nValidating val_graph:\")\n",
    "validate_edge_indices(val_graph)\n",
    "print(\"\\nValidating test_graph:\")\n",
    "validate_edge_indices(test_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroGNN(\n",
       "  (user_emb): Embedding(226570, 128)\n",
       "  (recipe_norm): BatchNorm(3081, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (recipe_lin): Linear(in_features=3081, out_features=128, bias=True)\n",
       "  (convs): ModuleList(\n",
       "    (0-1): 2 x HeteroConv(num_relations=2)\n",
       "  )\n",
       "  (batch_norms): ModuleList(\n",
       "    (0-1): 2 x BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (dropout_layer): Dropout(p=0.2, inplace=False)\n",
       "  (predict_mlp): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, graph, model_type, hidden_channels, num_layers, dropout, l2_reg):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type.lower()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        # User node_embeddings\n",
    "        user_node_count = graph['user'].num_nodes\n",
    "        self.user_emb = torch.nn.Embedding(user_node_count, self.hidden_channels)\n",
    "        torch.nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "\n",
    "        # Recipe Features\n",
    "        recipe_x_dim = graph['recipe'].x.size(-1)\n",
    "        self.recipe_norm = BatchNorm(recipe_x_dim, affine=True)\n",
    "        self.recipe_lin = torch.nn.Linear(recipe_x_dim, self.hidden_channels)\n",
    "        torch.nn.init.xavier_uniform_(self.recipe_lin.weight)\n",
    "\n",
    "        # Define Convolutional Layers and BatchNorms\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv_dict = {}\n",
    "            for edge_type in graph.edge_types:\n",
    "                src, rel, dst = edge_type\n",
    "                if self.model_type == 'sage':\n",
    "                    conv_dict[edge_type] = SAGEConv((-1, -1), self.hidden_channels)\n",
    "                elif self.model_type == 'gat':\n",
    "                    conv_dict[edge_type] = GATv2Conv((-1, -1), self.hidden_channels, heads=4, concat=False)\n",
    "                elif self.model_type == 'lightgcn':\n",
    "                    conv_dict[edge_type] = LGConv(normalize=True)\n",
    "                else:\n",
    "                    raise ValueError(\"model_type should be one of ['sage', 'gat', 'lightgcn']\")\n",
    "            self.convs.append(HeteroConv(conv_dict, aggr='mean'))\n",
    "\n",
    "            if self.model_type in ['sage', 'gat']:\n",
    "                self.batch_norms.append(BatchNorm(self.hidden_channels))\n",
    "            else:\n",
    "                self.batch_norms.append(torch.nn.Identity())  # No BatchNorm for LightGCN\n",
    "\n",
    "        # Dropout Layer\n",
    "        self.dropout_layer = torch.nn.Dropout(dropout) if dropout > 0 else None\n",
    "\n",
    "        # Prediction Layer (out_channels fixed to 1)\n",
    "        if self.model_type in ['sage', 'gat']:\n",
    "            self.predict_mlp = torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.hidden_channels * 2, self.hidden_channels),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(self.hidden_channels, 1)  # Output fixed to 1\n",
    "            )\n",
    "        else:\n",
    "            self.predict_mlp = None\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Replace user node features with node_embeddings\n",
    "        x_dict['user'] = self.user_emb.weight  # [num_users, hidden_channels]\n",
    "        x_dict['recipe'] = self.recipe_norm(x_dict['recipe'])  # [num_recipes, recipe_x_dim]\n",
    "        x_dict['recipe'] = self.recipe_lin(x_dict['recipe'])  # [num_recipes, hidden_channels]\n",
    "\n",
    "        # Apply HeteroConv layers\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            if self.model_type in ['sage', 'gat']:\n",
    "                # Apply BatchNorm, Dropout, and Activation only for GraphSage and GAT\n",
    "                x_dict = {key: bn(x) for key, x in x_dict.items()}\n",
    "                if self.dropout_layer:\n",
    "                    x_dict = {key: self.dropout_layer(x) for key, x in x_dict.items()}\n",
    "                x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "            else:\n",
    "                # For LightGCN, no BatchNorm, Dropout, or Activation\n",
    "                pass\n",
    "\n",
    "        # Collect final node_embeddings\n",
    "        out_dict = {\n",
    "            'user': x_dict['user'],     # [num_users, hidden_channels]\n",
    "            'recipe': x_dict['recipe']  # [num_recipes, hidden_channels]\n",
    "        }\n",
    "        return out_dict\n",
    "\n",
    "    def predict(self, user_emb, recipe_emb):\n",
    "        if self.model_type == 'lightgcn':\n",
    "            # LightGCN uses inner product for prediction\n",
    "            return (user_emb * recipe_emb).sum(dim=-1, keepdim=True)\n",
    "        else:\n",
    "            # GraphSage and GAT use MLP for prediction\n",
    "            combined = torch.cat([user_emb, recipe_emb], dim=-1)  # [batch_size, hidden_channels * 2]\n",
    "            return self.predict_mlp(combined)  # [batch_size, 1]\n",
    "\n",
    "    def loss_l2_regularization(self):\n",
    "        l2_loss = torch.sum(self.user_emb.weight**2)\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and param.requires_grad:\n",
    "                l2_loss += torch.sum(param**2)\n",
    "        return self.l2_reg * l2_loss\n",
    "\n",
    "# Model configuration\n",
    "model_type = 'sage'\n",
    "hidden_channels = 128\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "l2_reg = 1e-5\n",
    "\n",
    "# Initialize the model with the entire train_graph.\n",
    "model = HeteroGNN(\n",
    "    graph=train_graph, \n",
    "    model_type=model_type,\n",
    "    hidden_channels=hidden_channels, \n",
    "    num_layers=num_layers, \n",
    "    dropout=dropout,\n",
    "    l2_reg=l2_reg\n",
    ").to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training & Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aref/ar_code/food-recipe-recommendation/venv/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 001/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 2.4643, Validation MSE: 9.1168\n",
      "\n",
      "Epoch 002/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.5931, Validation MSE: 7.6989\n",
      "\n",
      "Epoch 003/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.5551, Validation MSE: 6.4364\n",
      "\n",
      "Epoch 004/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.5402, Validation MSE: 6.0248\n",
      "\n",
      "Epoch 005/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.5328, Validation MSE: 5.8965\n",
      "\n",
      "Epoch 006/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.5153, Validation MSE: 5.5993\n",
      "\n",
      "Epoch 007/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.5078, Validation MSE: 5.8933\n",
      "\n",
      "Epoch 008/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.4957, Validation MSE: 6.0221\n",
      "\n",
      "Epoch 009/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.4876, Validation MSE: 6.5956\n",
      "\n",
      "Epoch 010/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.4508, Validation MSE: 5.9384\n",
      "\n",
      "Epoch 011/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.4360, Validation MSE: 6.3885\n",
      "\n",
      "Epoch 012/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.4296, Validation MSE: 5.9184\n",
      "\n",
      "Epoch 013/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.3940, Validation MSE: 5.6621\n",
      "\n",
      "Epoch 014/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.3795, Validation MSE: 5.8883\n",
      "\n",
      "Epoch 015/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.3714, Validation MSE: 5.6372\n",
      "\n",
      "Epoch 016/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.3445, Validation MSE: 5.9938\n",
      "\n",
      "Epoch 017/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.3310, Validation MSE: 5.6955\n",
      "\n",
      "Epoch 018/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.3256, Validation MSE: 5.8750\n",
      "\n",
      "Epoch 019/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.3079, Validation MSE: 5.7306\n",
      "\n",
      "Epoch 020/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.3020, Validation MSE: 5.8171\n",
      "\n",
      "Epoch 021/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.2979, Validation MSE: 5.7183\n",
      "\n",
      "Epoch 022/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.2880, Validation MSE: 5.6965\n",
      "\n",
      "Epoch 023/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.2834, Validation MSE: 5.7577\n",
      "\n",
      "Epoch 024/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.2848, Validation MSE: 5.6827\n",
      "\n",
      "Epoch 025/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.2753, Validation MSE: 5.7214\n",
      "\n",
      "Epoch 026/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.2767, Validation MSE: 5.7442\n",
      "\n",
      "Epoch 027/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.2771, Validation MSE: 5.6662\n",
      "\n",
      "Epoch 028/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.2699, Validation MSE: 5.6602\n",
      "\n",
      "Epoch 029/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.2711, Validation MSE: 5.6698\n",
      "\n",
      "Epoch 030/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.2702, Validation MSE: 5.7083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch and computes training MSE.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (nn.Module): The GNN model.\n",
    "    - loader (LinkNeighborLoader): Data loader for training.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer.\n",
    "    - device (torch.device): Device to run computations on.\n",
    "    \n",
    "    Returns:\n",
    "    - average_mse (float): Average MSE over the training set.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    mse_sum = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Training\", unit=\"batch\", leave=False):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prepare node features\n",
    "        x_dict = {\n",
    "            nt: torch.arange(batch[nt].num_nodes, device=device) if nt == 'user' else batch[nt].x\n",
    "            for nt in ['user', 'recipe']\n",
    "        }\n",
    "\n",
    "        # Forward pass\n",
    "        out_dict = model(x_dict, batch.edge_index_dict)\n",
    "\n",
    "        # Extract edge information\n",
    "        edge = batch['user', 'rates', 'recipe']\n",
    "        user_emb = out_dict['user'][edge.edge_label_index[0]]\n",
    "        recipe_emb = out_dict['recipe'][edge.edge_label_index[1]]\n",
    "\n",
    "        # Prediction and target\n",
    "        pred = model.predict(user_emb, recipe_emb).squeeze()\n",
    "        target = edge.edge_label.float().squeeze()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(pred, target) + model.loss_l2_regularization()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate MSE\n",
    "        mse_sum += F.mse_loss(pred, target, reduction='sum').item()\n",
    "        count += target.size(0)\n",
    "\n",
    "    average_mse = mse_sum / count\n",
    "    return average_mse\n",
    "\n",
    "def evaluate_mse(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given data loader and computes MSE.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (nn.Module): The GNN model.\n",
    "    - loader (LinkNeighborLoader): Data loader for evaluation.\n",
    "    - device (torch.device): Device to run computations on.\n",
    "    \n",
    "    Returns:\n",
    "    - average_mse (float): Average MSE over the evaluation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    mse_sum = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\", unit=\"batch\", leave=False):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Prepare node features\n",
    "            x_dict = {\n",
    "                nt: torch.arange(batch[nt].num_nodes, device=device) if nt == 'user' else batch[nt].x\n",
    "                for nt in ['user', 'recipe']\n",
    "            }\n",
    "\n",
    "            # Forward pass\n",
    "            out_dict = model(x_dict, batch.edge_index_dict)\n",
    "\n",
    "            # Extract edge information\n",
    "            edge = batch['user', 'rates', 'recipe']\n",
    "            user_emb = out_dict['user'][edge.edge_label_index[0]]\n",
    "            recipe_emb = out_dict['recipe'][edge.edge_label_index[1]]\n",
    "\n",
    "            # Prediction and target\n",
    "            pred = model.predict(user_emb, recipe_emb).squeeze()\n",
    "            target = edge.edge_label.float().squeeze()\n",
    "\n",
    "            # Accumulate MSE\n",
    "            mse_sum += F.mse_loss(pred, target, reduction='sum').item()\n",
    "            count += target.size(0)\n",
    "\n",
    "    average_mse = mse_sum / count\n",
    "    return average_mse\n",
    "\n",
    "def create_link_neighbor_loader(data, edge_type, batch_size, num_neighbors, shuffle, num_workers):\n",
    "    if edge_type not in data.edge_types:\n",
    "        raise ValueError(f\"Edge type {edge_type} not found in the data.\")\n",
    "\n",
    "    edge_label_index = data[edge_type].edge_label_index\n",
    "    edge_label = data[edge_type].edge_label\n",
    "\n",
    "    loader = LinkNeighborLoader(\n",
    "        data=data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        edge_label_index=(edge_type, edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "# Define edge type for link prediction\n",
    "edge_type = ('user', 'rates', 'recipe')\n",
    "batch_size = 1024\n",
    "num_neighbors = [10] + [5] * (num_layers - 1)\n",
    "num_workers = 3\n",
    "\n",
    "# Create loaders\n",
    "train_loader = create_link_neighbor_loader(\n",
    "    data=train_graph,\n",
    "    edge_type=edge_type,\n",
    "    batch_size=batch_size,\n",
    "    num_neighbors=num_neighbors,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_loader = create_link_neighbor_loader(\n",
    "    data=val_graph,\n",
    "    edge_type=edge_type,\n",
    "    batch_size=batch_size,\n",
    "    num_neighbors=num_neighbors,\n",
    "    shuffle=False,  # No need to shuffle for evaluation\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30 \n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\nEpoch {epoch:03d}/{num_epochs}\")\n",
    "    \n",
    "    # Training phase\n",
    "    train_mse = train_epoch(model, train_loader, optimizer, device)\n",
    "    \n",
    "    # Evaluation phase\n",
    "    val_mse = evaluate_mse(model, val_loader, device)\n",
    "    \n",
    "    # Scheduler step based on validation MSE\n",
    "    scheduler.step(val_mse)\n",
    "    \n",
    "    print(f\"Train MSE: {train_mse:.4f}, Validation MSE: {val_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Finally, we evaluate the performance of our model on the validation and test graphs using the **Root Mean Squared Error (RMSE)** and **@Recall@k** metrics. \n",
    "Although MSE is used as a loss function for training due to its efficient gradient properties, we use RMSE for evaluation because it provides error values in the same units as the target variable, making it more interpretable when assessing model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 5.6514, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 5.1266, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0168,  ..., 0.0000, 5.2939, 0.0000],\n",
       "         ...,\n",
       "         [0.5076, 0.0000, 0.3351,  ..., 0.4859, 0.0460, 0.5466],\n",
       "         [0.0000, 0.0000, 0.6161,  ..., 0.0000, 5.6052, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 5.8774, 0.0000]],\n",
       "        device='cuda:0'),\n",
       " 'recipe': tensor([[0.6738, 0.0392, 0.0000,  ..., 0.0000, 0.7699, 0.0000],\n",
       "         [0.3316, 0.9225, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.7020, 0.1096, 1.0756,  ..., 0.0000, 0.4198, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0196, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 1.5273, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 1.2378, 0.0000, 0.0000]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_all_node_embeddings(model, data, device, node_types=['user', 'recipe']):\n",
    "    \"\"\"\n",
    "    Computes node_embeddings for all specified node types using a full forward pass.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): The trained GNN model.\n",
    "    - data (HeteroData): The entire graph data.\n",
    "    - device (torch.device): The device to perform computations on.\n",
    "    - node_types (list): List of node types to compute node_embeddings for.\n",
    "\n",
    "    Returns:\n",
    "    - node_embeddings_dict (dict): Dictionary mapping node types to their embeddings.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Prepare node features\n",
    "        x_dict = {}\n",
    "        for node_type in node_types:\n",
    "            if node_type == 'user':\n",
    "                # Use node indices as placeholders; model will replace them with embeddings\n",
    "                x_dict[node_type] = torch.arange(data[node_type].num_nodes, device=device)\n",
    "            else:\n",
    "                # Use actual features for other node types\n",
    "                x_dict[node_type] = data[node_type].x.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out_dict = model(x_dict, data.edge_index_dict)\n",
    "\n",
    "    node_embeddings_dict = {}\n",
    "    for node_type in node_types:\n",
    "        node_embeddings_dict[node_type] = out_dict[node_type]\n",
    "\n",
    "    return node_embeddings_dict\n",
    "\n",
    "\n",
    "node_embeddings = get_all_node_embeddings(\n",
    "    model=model,\n",
    "    data=train_graph,  # Ensure 'train_graph' includes all nodes\n",
    "    device=device,\n",
    "    node_types=['user', 'recipe']\n",
    ")\n",
    "\n",
    "node_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: RMSE = 2.4294, Recall@5 = 0.0931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: RMSE = 2.4333, Recall@5 = 0.0534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_by_rmse(model, data_loader, node_embeddings, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_edges = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc='Evaluating RMSE', leave=False):\n",
    "        edge_type = ('user', 'rates', 'recipe')  # Adjust if your edge type differs\n",
    "        edge_label_index = batch[edge_type].edge_label_index.to(device)  # [2, E]\n",
    "        edge_label = batch[edge_type].edge_label.to(device).view(-1)  # [E]\n",
    "\n",
    "        users = edge_label_index[0]    # [E]\n",
    "        recipes = edge_label_index[1]  # [E]\n",
    "\n",
    "        user_emb = node_embeddings['user'][users]        # [E, hidden_dim]\n",
    "        recipe_emb = node_embeddings['recipe'][recipes]  # [E, hidden_dim]\n",
    "\n",
    "        # Generate predictions\n",
    "        pred = model.predict(user_emb, recipe_emb).squeeze()\n",
    "        pred = pred.clamp(min=0, max=5)  # Clamp predictions\n",
    "\n",
    "        # Compute MSE\n",
    "        mse = F.mse_loss(pred, edge_label, reduction='sum').item()\n",
    "        total_loss += mse\n",
    "        total_edges += edge_label.size(0)\n",
    "\n",
    "    average_rmse = (total_loss / total_edges) ** 0.5\n",
    "    return average_rmse\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_by_recall_at_k(model, data_loader, node_embeddings, k, relevance_threshold, device):\n",
    "    model.eval()\n",
    "    user_predictions = defaultdict(list)\n",
    "    user_true_items = defaultdict(set)\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=f\"Evaluating Recall@{k}\", leave=False):\n",
    "        edge_type = ('user', 'rates', 'recipe')  # Adjust if your edge type differs\n",
    "        edge_label_index = batch[edge_type].edge_label_index.to(device)  # [2, E]\n",
    "        edge_label = batch[edge_type].edge_label.to(device).view(-1)  # [E]\n",
    "\n",
    "        users = edge_label_index[0]    # [E]\n",
    "        recipes = edge_label_index[1]  # [E]\n",
    "\n",
    "        user_emb = node_embeddings['user'][users]        # [E, hidden_dim]\n",
    "        recipe_emb = node_embeddings['recipe'][recipes]  # [E, hidden_dim]\n",
    "\n",
    "        # Generate predictions\n",
    "        pred = model.predict(user_emb, recipe_emb).squeeze()\n",
    "        pred = pred.clamp(min=0, max=5)  # Clamp predictions\n",
    "\n",
    "        # Populate user_predictions and user_true_items\n",
    "        for i, user_id in enumerate(users.cpu().numpy()):\n",
    "            user_predictions[user_id].append((pred[i].item(), recipes[i].item()))\n",
    "            if edge_label[i].item() >= relevance_threshold:\n",
    "                user_true_items[user_id].add(recipes[i].item())\n",
    "\n",
    "    recalls = []\n",
    "    for user_id in user_predictions:\n",
    "        # Sort predictions by score in descending order and get top-k items\n",
    "        sorted_predictions = sorted(user_predictions[user_id], key=lambda x: x[0], reverse=True)\n",
    "        topk_recipes = {item for _, item in sorted_predictions[:k]}\n",
    "        true_items = user_true_items[user_id]\n",
    "\n",
    "        if true_items:\n",
    "            recall = len(topk_recipes & true_items) / len(true_items)\n",
    "            recalls.append(recall)\n",
    "\n",
    "    recall_at_k = sum(recalls) / len(recalls) if recalls else 0.0\n",
    "    return recall_at_k\n",
    "\n",
    "\n",
    "def create_link_neighbor_loader(data, edge_type, batch_size, num_neighbors, shuffle, num_workers):\n",
    "    if edge_type not in data.edge_types:\n",
    "        raise ValueError(f\"Edge type {edge_type} not found in the data.\")\n",
    "\n",
    "    edge_label_index = data[edge_type].edge_label_index\n",
    "    edge_label = data[edge_type].edge_label\n",
    "\n",
    "    loader = LinkNeighborLoader(\n",
    "        data=data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        edge_label_index=(edge_type, edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "test_loader = create_link_neighbor_loader(\n",
    "    data=test_graph,\n",
    "    edge_type=edge_type,\n",
    "    batch_size=batch_size,\n",
    "    num_neighbors=num_neighbors,\n",
    "    shuffle=False,        # No need to shuffle for evaluation\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# List of evaluation datasets and their corresponding loaders\n",
    "evaluation_sets = [\n",
    "    ('Validation', val_graph, val_loader),\n",
    "    ('Test', test_graph, test_loader)\n",
    "]\n",
    "\n",
    "# Define standalone variables for recall evaluation\n",
    "k = 5\n",
    "rating_threshold = 4 \n",
    "\n",
    "# Evaluation for Recall@K and RMSE\n",
    "for data_split, graph, loader in evaluation_sets:\n",
    "    # Compute RMSE.\n",
    "    rmse = evaluate_by_rmse(\n",
    "        model=model,\n",
    "        data_loader=loader,\n",
    "        node_embeddings=node_embeddings,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Compute Recall@K.\n",
    "    recall = evaluate_by_recall_at_k(\n",
    "        model=model,\n",
    "        data_loader=loader,\n",
    "        node_embeddings=node_embeddings,\n",
    "        k=k,\n",
    "        relevance_threshold=rating_threshold,\n",
    "        device=device\n",
    "    )\n",
    "        \n",
    "    print(f\"{data_split} set: RMSE = {rmse:.4f}, Recall@{k} = {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

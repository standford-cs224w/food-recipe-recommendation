{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGCN-based Recommender Model (LightGCNRecModel) for Food Recipe Recommendation (V0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import LGConv, to_hetero\n",
    "from torch_geometric.loader import LinkNeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flush():\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "flush()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph dataset loading\n",
    "\n",
    "In this step, we load the graphs already generated in the graph dataset generation step.    \n",
    "Since generating graph datasets is time consiming, we won't add them to each notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_428818/193544470.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  user={ num_nodes=226570 },\n",
       "  recipe={ x=[231637, 3081] },\n",
       "  (user, rates, recipe)={\n",
       "    edge_index=[2, 770011],\n",
       "    edge_label=[192502, 1],\n",
       "    edge_label_index=[2, 192502],\n",
       "  },\n",
       "  (recipe, rev_rates, user)={ edge_index=[2, 770011] }\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_graph(file_path):\n",
    "    return torch.load(file_path)\n",
    "\n",
    "dataset_version = 1\n",
    "base_data_path = f\"../data/graph/v{dataset_version}\"\n",
    "\n",
    "train_graph = load_graph(f\"{base_data_path}/train_graph.pt\")\n",
    "validation_graph = load_graph(f\"{base_data_path}/validation_graph.pt\")\n",
    "test_graph = load_graph(f\"{base_data_path}/test_graph.pt\")\n",
    "\n",
    "train_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train graph information: \n",
      "Numbder of nodes: 458207\n",
      "Numbder of edges: 1540022\n",
      "Metadata: (['user', 'recipe'], [('user', 'rates', 'recipe'), ('recipe', 'rev_rates', 'user')])\n",
      "Edge index: tensor([[  3106,    317,  16543,  ...,    541, 208023,    489],\n",
      "        [211809,   6600, 109688,  ...,  62108,  96459, 200804]])\n",
      "Recipe embeddings dimension:  3081\n",
      "torch.int64\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train graph information: \")\n",
    "print(\"Numbder of nodes:\", train_graph.num_nodes)\n",
    "print(\"Numbder of edges:\", train_graph.num_edges)\n",
    "print(\"Metadata:\", train_graph.metadata())\n",
    "print(\"Edge index:\", train_graph['user', 'rates', 'recipe'].edge_index)\n",
    "print(\"Recipe embeddings dimension: \", train_graph['recipe'].x.size(1))\n",
    "print(train_graph[('user', 'rates', 'recipe')].edge_index.dtype)  # Should be torch.long\n",
    "print(train_graph[('user', 'rates', 'recipe')].edge_label_index.dtype)  # Should be torch.long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LightGCNRecModel(\n",
       "  (user_embedding): Embedding(226570, 32)\n",
       "  (recipe_transform): Linear(in_features=3081, out_features=32, bias=True)\n",
       "  (encoder): GraphModule(\n",
       "    (convs): ModuleList(\n",
       "      (0-2): 3 x ModuleDict(\n",
       "        (user__rates__recipe): LGConv()\n",
       "        (recipe__rev_rates__user): LGConv()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): EdgeDecoder()\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LightGCN-like encoder using LGConv without normalization\n",
    "class LightGCNEncoder(nn.Module):\n",
    "    def __init__(self, num_conv_layers):\n",
    "        super().__init__()\n",
    "        # Disable normalization inside LGConv\n",
    "        # This avoids calling gcn_norm which expects a single tensor x.\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.convs = nn.ModuleList([LGConv(normalize=False) for _ in range(num_conv_layers)])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x: Node embedding matrix or tuple of node embedding matrices\n",
    "        all_out = x\n",
    "        out = x\n",
    "        for conv in self.convs:\n",
    "            out = conv(out, edge_index)\n",
    "            all_out += out\n",
    "        out = all_out / (self.num_conv_layers + 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Updated EdgeDecoder with dot product\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        \"\"\"\n",
    "        Predict edge scores (ratings) using dot product.\n",
    "        \"\"\"\n",
    "        row, col = edge_label_index\n",
    "        user_feat = z_dict['user'][row]\n",
    "        recipe_feat = z_dict['recipe'][col]\n",
    "        pred = (user_feat * recipe_feat).sum(dim=-1)\n",
    "        return pred\n",
    "\n",
    "\n",
    "class LightGCNRecModel(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        num_users = args['num_users']\n",
    "        item_feature_dim = args['item_feature_dim']\n",
    "        hidden_dim = args['hidden_dim']\n",
    "        num_conv_layers = args['num_conv_layers']\n",
    "        dropout_rate = args['dropout_rate']\n",
    "\n",
    "        self.user_embedding = nn.Embedding(num_users, hidden_dim)\n",
    "        self.recipe_transform = nn.Linear(item_feature_dim, hidden_dim)\n",
    "        \n",
    "        self.encoder = LightGCNEncoder(num_conv_layers)\n",
    "        self.encoder = to_hetero(self.encoder, train_graph.metadata(), aggr='sum')\n",
    "        \n",
    "        self.decoder = EdgeDecoder()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self._initialize_embeddings()\n",
    "\n",
    "    def _initialize_embeddings(self):\n",
    "        torch.nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        self.user_embedding.weight.data = F.normalize(self.user_embedding.weight.data, p=2, dim=1)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        x_dict['user'] = F.normalize(self.user_embedding.weight, p=2, dim=1)\n",
    "        x_dict['recipe'] = F.normalize(x_dict['recipe'], p=2, dim=1)\n",
    "        x_dict['recipe'] = self.recipe_transform(x_dict['recipe']).relu()\n",
    "        x_dict['recipe'] = self.dropout(x_dict['recipe'])\n",
    "\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "\n",
    "        return self.decoder(z_dict, edge_label_index)\n",
    "\n",
    "\n",
    "# Edge indices\n",
    "edge_index = train_graph[('user', 'rates', 'recipe')].edge_index\n",
    "edge_label_index = train_graph[('user', 'rates', 'recipe')].edge_label_index\n",
    "edge_label = train_graph[('user', 'rates', 'recipe')].edge_label\n",
    "\n",
    "model_args = {\n",
    "    'num_users': train_graph['user'].num_nodes,\n",
    "    'item_feature_dim': train_graph['recipe'].x.size(1),\n",
    "    'hidden_dim': 32,\n",
    "    'num_conv_layers': 3,\n",
    "    'dropout_rate': 0,\n",
    "}\n",
    "\n",
    "# Initialize the model.\n",
    "model = LightGCNRecModel(model_args).to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "In this step, we train our initialized \\texttt{SageRecModel} model to optimize for the edge rating prediction task, where we predict user ratings for recipes.\n",
    "\n",
    "### Mini-Batching\n",
    "To manage memory and computation on our large graph datasets, we use mini-batching, dividing the data into smaller, manageable subsets that the model processes sequentially. With PyG’s `LinkNeighborLoader`, we sample neighbors around each target edge, focusing on the local neighborhood of each user-recipe interaction. This approach enables the model to capture essential neighborhood context without loading the entire graph, making it highly efficient for large-scale training.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "- **Optimizer**: To optimize the model’s parameters, we use Adam optimizer that handles sparse gradients well, suitable for graph neural networks. Regularization is applied through weight decay, which discourages complex solutions and helps prevent overfitting, leading to better generalization.  \n",
    "To further stabilize training, we use a learning rate scheduler that dynamically adjusts the learning rate, reducing it gradually as the model approaches convergence. This prevents overshooting during optimization and enables fine-tuning for more accurate predictions.\n",
    "\n",
    "\n",
    "- **Loss Function**: Mean Squared Error (MSE) loss measures the difference between predicted and actual ratings. Given the continuous nature of ratings, MSE is a suitable choice for our link regression task and is calculated as:\n",
    "\n",
    "  $$\n",
    "  \\text{MSE Loss} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "\n",
    "  where $y_i$ is the true rating, $\\hat{y}_i$ is the predicted rating, and $N$ is the number of samples.\n",
    "\n",
    "\n",
    "### Training Process\n",
    "For each epoch, we perform the following steps:\n",
    "  1. Batch sampling: Sampling mini-batches of edges with the train data loader, allowing the model to process a portion of the data at each step.\n",
    "  2. Forward propagation: Generating predictions for each mini-batch and calculating the MSE loss based on the difference between predicted and actual ratings.\n",
    "  3. Backward Ppropagation: Updating the model’s parameters based on the computed gradients.\n",
    "  4. Parameter update: Updating the model’s parameters based on the computed gradients by optimizer.\n",
    "  5. Learning rate adjustment: Adjusting the learning rate periodically with the scheduler, stabilizing training as the model converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aref/ar_code/food-recipe-recommendation/venv/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    }
   ],
   "source": [
    "def create_link_neighbor_loader(data, edge_type, batch_size, num_neighbors, shuffle, num_workers):\n",
    "    \"\"\"\n",
    "    Creates a LinkNeighborLoader for the specified edge type in a HeteroData object.\n",
    "\n",
    "    Parameters:\n",
    "    - data (HeteroData): The heterogeneous graph data.\n",
    "    - edge_type (tuple): The edge type for which to create the loader, e.g., ('user', 'rates', 'recipe').\n",
    "    - batch_size (int): Number of edges to include in each batch.\n",
    "    - num_neighbors (list): Number of neighbors to sample at each layer.\n",
    "    - shuffle (bool): Whether to shuffle the data.\n",
    "    - num_workers (int): Number of subprocesses to use for data loading.\n",
    "\n",
    "    Returns:\n",
    "    - loader (LinkNeighborLoader): The data loader for the specified edge type.\n",
    "    \"\"\"\n",
    "    # Ensure the edge_type exists in the data\n",
    "    if edge_type not in data.edge_types:\n",
    "        raise ValueError(f\"Edge type {edge_type} not found in the data.\")\n",
    "\n",
    "    # Access the edge_label_index and edge_label for the specified edge type\n",
    "    edge_label_index = data[edge_type].get('edge_label_index', data[edge_type].edge_index)\n",
    "    edge_label = data[edge_type].get('edge_label', None)\n",
    "\n",
    "    # Create the LinkNeighborLoader\n",
    "    loader = LinkNeighborLoader(\n",
    "        data=data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        edge_label_index=(edge_type, edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "edge_type = ('user', 'rates', 'recipe') # Define the edge type of interest\n",
    "batch_size = 512  # Adjust based on your GPU memory capacity\n",
    "num_neighbors = [10, 5, 5] # Number of neighbors to sample at each layer\n",
    "num_workers = 4  # Adjust based on your system\n",
    "\n",
    "# Create the training data loader\n",
    "train_data_loader = create_link_neighbor_loader(\n",
    "    data=train_graph,\n",
    "    edge_type=edge_type,\n",
    "    batch_size=batch_size, \n",
    "    num_neighbors=num_neighbors, \n",
    "    shuffle=True,\n",
    "    num_workers=num_workers \n",
    ")\n",
    "\n",
    "weight_decay = 0.0001\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Implement a learning rate scheduler\n",
    "step_size = 10\n",
    "gamma = 0.1\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss (MSE): 14.7066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss (MSE): 12.8996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def train(model, data_loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc='Training', unit='batch', leave=False):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch.x_dict, batch.edge_index_dict, batch['user', 'rates', 'recipe'].edge_label_index)\n",
    "        # Flatten target to match pred.\n",
    "        target = batch['user', 'rates', 'recipe'].edge_label.float().view(-1)\n",
    "        loss = F.mse_loss(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * target.size(0)\n",
    "    \n",
    "    # Compute average loss (MSE) per data point (edge). \n",
    "    mse = total_loss / len(data_loader.dataset)\n",
    "\n",
    "    # Step the scheduler to update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return mse\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train(model, train_data_loader, optimizer, scheduler)\n",
    "    print(f'Epoch: {epoch:03d}, Loss (MSE): {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Finally, we evaluate the performance of our model on the validation and test graphs using the **Root Mean Squared Error (RMSE)** and **@Recall@k** metrics. \n",
    "Although MSE is used as a loss function for training due to its efficient gradient properties, we use RMSE for evaluation because it provides error values in the same units as the target variable, making it more interpretable when assessing model performance.\n",
    "\n",
    "We also report the evaluation results on the training graph to monitor the model error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: RMSE = 3.5326, Recall@5 = 0.9642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: RMSE = 3.6303, Recall@5 = 0.9583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def encode_graph(model, graph, device):\n",
    "    \"\"\"\n",
    "    Precomputes node embeddings for the full graph.\n",
    "    \"\"\"\n",
    "    graph = graph.to(device)\n",
    "    x_dict = graph.x_dict\n",
    "    edge_index_dict = graph.edge_index_dict\n",
    "\n",
    "    # Normalize and precompute embeddings\n",
    "    x_dict['user'] = F.normalize(model.user_embedding.weight, p=2, dim=1)\n",
    "    x_dict['recipe'] = F.normalize(model.recipe_transform(x_dict['recipe']), p=2, dim=1)\n",
    "    return model.encoder(x_dict, edge_index_dict)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_rmse(decoder, z_dict, edge_label_index, edge_label, batch_size, device):\n",
    "    \"\"\"\n",
    "    Computes RMSE for edge predictions.\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    total_edges = 0\n",
    "\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    for i in tqdm(range(0, edge_label_index.size(1), batch_size), desc=\"RMSE\", leave=False):\n",
    "        batch_edge_index = edge_label_index[:, i:i+batch_size]\n",
    "        batch_edge_label = edge_label[i:i+batch_size].to(device)\n",
    "        \n",
    "        # Remove extra dimension to match predictions\n",
    "        batch_edge_label = batch_edge_label.view(-1)\n",
    "        \n",
    "        pred = decoder(z_dict, batch_edge_index).clamp(min=0, max=5)\n",
    "        total_loss += F.mse_loss(pred, batch_edge_label, reduction='sum').item()\n",
    "        total_edges += batch_edge_label.size(0)\n",
    "\n",
    "    return (total_loss / total_edges) ** 0.5\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_recall_at_k(decoder, z_dict, edge_label_index, edge_label, batch_size, k, relevance_threshold, device):\n",
    "    \"\"\"\n",
    "    Computes Recall@K for edge predictions.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    user_predictions = defaultdict(list)\n",
    "    user_true_items = defaultdict(set)\n",
    "\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    for i in tqdm(range(0, edge_label_index.size(1), batch_size), desc=\"Recall@K\", leave=False):\n",
    "        batch_edge_index = edge_label_index[:, i:i+batch_size]\n",
    "        batch_edge_label = edge_label[i:i+batch_size].to(device)\n",
    "        \n",
    "        # Remove extra dimension to match predictions\n",
    "        batch_edge_label = batch_edge_label.view(-1)\n",
    "        \n",
    "        pred = decoder(z_dict, batch_edge_index).clamp(min=0, max=5)\n",
    "\n",
    "        users, items = batch_edge_index\n",
    "        for j, user_id in enumerate(users.cpu().numpy()):\n",
    "            user_predictions[user_id].append((pred[j].item(), items[j].item()))\n",
    "            if batch_edge_label[j].item() >= relevance_threshold:\n",
    "                user_true_items[user_id].add(items[j].item())\n",
    "\n",
    "    recalls = []\n",
    "    for user_id in user_predictions:\n",
    "        top_k_pred_items = {item for _, item in sorted(user_predictions[user_id], key=lambda x: x[0], reverse=True)[:k]}\n",
    "        true_items = user_true_items[user_id]\n",
    "        if true_items:\n",
    "            recalls.append(len(top_k_pred_items & true_items) / len(true_items))\n",
    "    return sum(recalls) / len(recalls) if recalls else 0.0\n",
    "\n",
    "\n",
    "k = 5\n",
    "relevance_threshold = 4\n",
    "model.eval()\n",
    "\n",
    "for split_name, graph in zip([\"Validation\", \"Test\"], [validation_graph, test_graph]):\n",
    "    z_dict = encode_graph(model, graph, device)\n",
    "\n",
    "    edge_label_index = graph['user', 'rates', 'recipe'].edge_label_index\n",
    "    edge_label = graph['user', 'rates', 'recipe'].edge_label.float()\n",
    "\n",
    "    rmse = calculate_rmse(model.decoder, z_dict, edge_label_index, edge_label, batch_size, device)\n",
    "    recall_at_k = calculate_recall_at_k(model.decoder, z_dict, edge_label_index, edge_label, batch_size, k, relevance_threshold, device)\n",
    "\n",
    "    print(f\"{split_name}: RMSE = {rmse:.4f}, Recall@{k} = {recall_at_k:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
